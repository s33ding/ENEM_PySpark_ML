{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import pyspark \n",
    "import os \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, col, udf, rank, asc, sum as spark_sum\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a Spark session\n",
    "spark = SparkSession.builder.appName(\"s33ding\").getOrCreate()\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = spark.read.parquet(\"dataset/enem.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/14 00:27:13 WARN Instrumentation: [4e50c318] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select the relevant columns\n",
    "selected_cols = [\"NOTA_CH_CIENCIAS_HUMANAS\", \"NOTA_LC_LINGUAGENS_E_CODIGOS\", \"NOTA_MT_MATEMATICA\", \"NOTA_REDACAO\"]\n",
    "df_selected = df.select(selected_cols)\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "train_data, validation_data, test_data = df_selected.randomSplit([0.6, 0.2, 0.2], seed=42)\n",
    "\n",
    "# Prepare the feature vector and the target column for the training, validation, and testing sets\n",
    "assembler = VectorAssembler(inputCols=[\"NOTA_LC_LINGUAGENS_E_CODIGOS\", \"NOTA_MT_MATEMATICA\", \"NOTA_REDACAO\"], outputCol=\"features\")\n",
    "\n",
    "train_data = assembler.transform(train_data).select(\"features\", \"NOTA_CH_CIENCIAS_HUMANAS\")\n",
    "validation_data = assembler.transform(validation_data).select(\"features\", \"NOTA_CH_CIENCIAS_HUMANAS\")\n",
    "test_data = assembler.transform(test_data).select(\"features\", \"NOTA_CH_CIENCIAS_HUMANAS\")\n",
    "\n",
    "# Train the machine learning models using the training and validation sets\n",
    "lr = LinearRegression(labelCol=\"NOTA_CH_CIENCIAS_HUMANAS\")\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "dt = DecisionTreeRegressor(labelCol=\"NOTA_CH_CIENCIAS_HUMANAS\")\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"NOTA_CH_CIENCIAS_HUMANAS\")\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions using the trained models on the validation set\n",
    "lr_predictions = lr_model.transform(validation_data)\n",
    "dt_predictions = dt_model.transform(validation_data)\n",
    "rf_predictions = rf_model.transform(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+------------------+------------------+------------------+\n",
      "|            features|NOTA_CH_CIENCIAS_HUMANAS|        prediction|     dt_prediction|     rf_prediction|\n",
      "+--------------------+------------------------+------------------+------------------+------------------+\n",
      "| [413.0,444.0,360.0]|                   311.6| 420.0014176407547| 429.0985439250563|432.19727563223535|\n",
      "|[345.399993896484...|                   314.7| 371.4723714050142| 415.3153703219153|423.97934549882996|\n",
      "|[326.700012207031...|                   319.5| 366.4658276543193| 415.3153703219153| 435.4225155561647|\n",
      "|[385.200012207031...|                   319.8|  386.307863024124| 415.3153703219153|426.84003556090846|\n",
      "|[334.600006103515...|                   323.1|393.97663016879727|439.61351666538144|453.77265998940663|\n",
      "|[434.200012207031...|                   323.7|  467.787728390522|439.61351666538144| 467.5242495362507|\n",
      "|[498.0,458.299987...|                   324.5|482.86925088372567|487.52015789468874| 468.3124981560103|\n",
      "|[389.700012207031...|                   325.9| 402.7945645796057|439.61351666538144| 443.8276377207956|\n",
      "|[351.700012207031...|                   327.7| 387.1674202540139| 415.3153703219153| 430.6857193133371|\n",
      "|[336.399993896484...|                   328.4|353.91622220781727| 415.3153703219153| 424.6043754836607|\n",
      "|[380.0,380.700012...|                   328.4|379.27835931306134| 415.3153703219153| 423.8150512656848|\n",
      "|[388.600006103515...|                   328.4| 377.6341670721584| 415.3153703219153| 423.2985902463491|\n",
      "|[486.600006103515...|                   328.4|  465.636424777179| 448.0761030851922| 459.7665111767771|\n",
      "|[551.299987792968...|                   328.4| 545.0399489101583| 548.4009132087307| 536.8351979072066|\n",
      "|[462.100006103515...|                   328.5| 431.4379309690517| 448.0761030851922| 442.2407950184802|\n",
      "|[463.399993896484...|                   328.5| 452.7682675406084| 448.0761030851922| 449.6520401994937|\n",
      "|[426.200012207031...|                   328.9|407.27225636875573| 429.0985439250563| 427.4146346163673|\n",
      "|[483.200012207031...|                   328.9|463.47336385137606| 448.0761030851922| 454.8598055476415|\n",
      "|[567.5,598.299987...|                   328.9| 553.0565545760629| 573.0388742417687| 548.5344509435911|\n",
      "|[332.700012207031...|                   329.1| 376.4097673055572| 415.3153703219153|441.24045258184316|\n",
      "+--------------------+------------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create temporary views for the predictions\n",
    "lr_predictions.createOrReplaceTempView(\"lr_predictions\")\n",
    "dt_predictions.createOrReplaceTempView(\"dt_predictions\")\n",
    "rf_predictions.createOrReplaceTempView(\"rf_predictions\")\n",
    "\n",
    "joined_predictions = spark.sql(\"\"\"\n",
    "    SELECT lr_predictions.*, dt_predictions.prediction AS dt_prediction, rf_predictions.prediction AS rf_prediction\n",
    "    FROM lr_predictions\n",
    "    JOIN dt_predictions ON lr_predictions.features = dt_predictions.features\n",
    "    JOIN rf_predictions ON lr_predictions.features = rf_predictions.features\n",
    "    ORDER BY NOTA_CH_CIENCIAS_HUMANAS\n",
    "\"\"\")\n",
    "\n",
    "# Get a sample from the joined_predictions DataFrame\n",
    "# Get a sample from the joined_predictions DataFrame\n",
    "sample_joined_predictions = joined_predictions.sample(fraction=0.1, seed=42)\n",
    "\n",
    "# Save the sample as a Parquet file\n",
    "sample_joined_predictions.write.mode('overwrite').parquet('data_for_dashboards/models/joined_predictions.parquet', mode='overwrite')\n",
    "\n",
    "# Show the joined predictions\n",
    "joined_predictions.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"NOTA_CH_CIENCIAS_HUMANAS\", predictionCol=\"prediction\")\n",
    "\n",
    "# Calculate MSE for linear regression\n",
    "lr_mse = evaluator.evaluate(lr_predictions, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# Calculate MSE for decision tree regression\n",
    "dt_mse = evaluator.evaluate(dt_predictions, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# Calculate MSE for random forest regression\n",
    "rf_mse = evaluator.evaluate(rf_predictions, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# Calculate MAE for linear regression\n",
    "lr_mae = evaluator.evaluate(lr_predictions, {evaluator.metricName: \"mae\"})\n",
    "\n",
    "# Calculate MAE for decision tree regression\n",
    "dt_mae = evaluator.evaluate(dt_predictions, {evaluator.metricName: \"mae\"})\n",
    "\n",
    "# Calculate MAE for random forest regression\n",
    "rf_mae = evaluator.evaluate(rf_predictions, {evaluator.metricName: \"mae\"})\n",
    "\n",
    "# Calculate MSE and MAE for each model\n",
    "metrics = [(\"Linear Regression - MSE\", evaluator.evaluate(lr_predictions, {evaluator.metricName: \"mse\"})),\n",
    "           (\"Decision Tree Regression - MSE\", evaluator.evaluate(dt_predictions, {evaluator.metricName: \"mse\"})),\n",
    "           (\"Random Forest Regression - MSE\", evaluator.evaluate(rf_predictions, {evaluator.metricName: \"mse\"})),\n",
    "           (\"Linear Regression - MAE\", evaluator.evaluate(lr_predictions, {evaluator.metricName: \"mae\"})),\n",
    "           (\"Decision Tree Regression - MAE\", evaluator.evaluate(dt_predictions, {evaluator.metricName: \"mae\"})),\n",
    "           (\"Random Forest Regression - MAE\", evaluator.evaluate(rf_predictions, {evaluator.metricName: \"mae\"}))]\n",
    "\n",
    "# Create the DataFrame with \"Type\" column\n",
    "df_comparing_models = spark.createDataFrame(metrics, [\"Model\", \"Metric\"]).withColumn(\"Type\", udf(lambda model: \"MSE\" if \"MSE\" in model else \"MAE\", StringType())(\"Model\"))\n",
    "\n",
    "# Create a window specification to partition by the type and order by the metric in ascending order\n",
    "window_spec = Window.partitionBy(\"Type\").orderBy(asc(\"Metric\"))\n",
    "\n",
    "# Add a \"Best\" column to determine the best model for each type\n",
    "df_comparing_models = df_comparing_models.withColumn(\"Best\", rank().over(window_spec) == 1)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_comparing_models.show(truncate=False)\n",
    "tmp = df_comparing_models.toPandas()\n",
    "tmp.to_parquet('data_for_dashboards/eda/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to save the model\n",
    "model_path = \"models/linear_regression_model\"\n",
    "\n",
    "# Save the Linear Regression model\n",
    "lr_model.save(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
